{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8845909d",
   "metadata": {},
   "source": [
    "## Listing of Noun Phrases From Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700d9f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dateutil\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "from bson import json_util\n",
    "\n",
    "import nltk\n",
    "import nltk.tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0da45",
   "metadata": {},
   "source": [
    "Same DB variables setup. This time I'm using the joined sample of 10, with the collection name `cleanedJoinedSample10`\n",
    "\n",
    "*Most print() calls are commented out because its way too big, but these are for debugging*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7138c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "mydb = client[\"amazonPhones\"]\n",
    "joinedSample10Collection = mydb[\"cleanedJoinedSample10\"]\n",
    "\n",
    "testDocument = joinedSample10Collection.find_one()\n",
    "#print(testDocument.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9f804",
   "metadata": {},
   "source": [
    "### addNounPhrasesCountField function\n",
    "This function is to be performed on 1 individual review to add a dictionary containing all the noun phrases in its `reviewText` and their count (num of times used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b80c7a",
   "metadata": {},
   "source": [
    "3 different grammar regex strings (this is a NL processing term).\n",
    "supposedly, all of them split the sentence into chunks of noun phrases (using different methods(?))\n",
    "\n",
    "googled a bunch of grammar strings and the 3rd one seems to have the least retarded results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f9b27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on first review\n",
    "testReview = testDocument['review'][1]\n",
    "# print(testReview)\n",
    "def addNounPhrasesCountField(review):\n",
    "    \n",
    "    review['nounPhrasesCount'] = {}\n",
    "\n",
    "    \n",
    "    if 'reviewText' not in review:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    testReviewText = review['reviewText']\n",
    "\n",
    "    # Tokenising sentence\n",
    "    word_tokenize = nltk.tokenize.word_tokenize\n",
    "    tokens = word_tokenize(testReviewText)\n",
    "#     print(tokens)\n",
    "\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "#     gram = (\"NP: {<DT>?<JJ>*<NN>}\")\n",
    "#     gram = r\"\"\"NP: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" # more specific & effective noun phrase identifier\n",
    "\n",
    "    gram = r\"NP: {<DT|PRP\\$>?<JJ>*<NN.*>+}\"\n",
    "    \n",
    "    \n",
    "    chunkParser = nltk.RegexpParser(gram)\n",
    "    parsed_tree = chunkParser.parse(tagged_tokens)\n",
    "\n",
    "    # print(parsed_tree)\n",
    "\n",
    "    # print(\"SUBTREES ---------------------\")\n",
    "\n",
    "    # for tree in parsed_tree.subtrees():\n",
    "    #     for leaf in tree.leaves():\n",
    "    #         print(leaf)\n",
    "\n",
    "\n",
    "    # each Leaf object contains [0] the word; and [1] the word category (eg. \"something / NN\")\n",
    "\n",
    "    noun_phrases = [ ' '.join(leaf[0] for leaf in tree.leaves()).lower() #have to lower here\n",
    "                    for tree in parsed_tree.subtrees() #        idk why if i lower earlier the parser fks up\n",
    "                    if tree.label() == 'NP']\n",
    "\n",
    "#     print(noun_phrases)\n",
    "\n",
    "    counts = dict()\n",
    "    for np in noun_phrases:\n",
    "      counts[np] = counts.get(np, 0) + 1\n",
    "    review['nounPhrasesCount'] = counts\n",
    "#     print(counts)\n",
    "#     print(review)\n",
    "\n",
    "# addNounPhrasesCountField(testReview)\n",
    "\n",
    "# print(testReview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe0816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('6415a4959cb8dce7838b34e5'), 'overall': 5, 'vote': '32', 'verified': True, 'reviewTime': '09 8, 2015', 'reviewerID': 'A2JFID6PCLJPO6', 'asin': 'B00YD547Q6', 'style': {'Color:': ' Space Gray'}, 'reviewerName': 'Jahdale Logan', 'reviewText': 'PERFECT CONDITION', 'summary': 'Ok', 'unixReviewTime': 1441670400, 'nounPhrasesCount': {'perfect condition': 1}}\n"
     ]
    }
   ],
   "source": [
    "#test on review 3 (actually 4 since index 0)\n",
    "reviewThree = testDocument['review'][3]\n",
    "\n",
    "addNounPhrasesCountField(reviewThree)\n",
    "\n",
    "print(reviewThree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54b933",
   "metadata": {},
   "source": [
    "### gatherAllReviewNPCount\n",
    "This is a function to be performed on 1 product document (basically 1 amazon product item).\n",
    "It loops through all the product reviews and applies the previous `addNounPhrasesCountField` function to each review, and then during each review run, it adds to its own `totalNounPhrasesCount` dictionary.\n",
    "\n",
    "(the `howMany` parameter is just for debugging, sth went wrong at review \\#600+ because `reviewText` did not exist for that particular review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38643c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherAllReviewsNPCount(productDocument, howMany=0):\n",
    "    productReviews = productDocument['review']\n",
    "    productDocument['totalNounPhrasesCount'] = {}\n",
    "    productDocumentCount = productDocument['totalNounPhrasesCount'] # just to make it shorter\n",
    "#     for rev in productReviews:\n",
    "\n",
    "#     doneCount = 0\n",
    "    if (howMany != 0):\n",
    "        for x in range(howMany):\n",
    "            rev = productReviews[x]\n",
    "            addNounPhrasesCountField(rev)\n",
    "            #test and do for first 4 first\n",
    "\n",
    "            for np in rev['nounPhrasesCount']:\n",
    "                productDocumentCount[np] = productDocumentCount.get(np, 0) + rev['nounPhrasesCount'][np]\n",
    "    else:\n",
    "        for rev in productReviews:\n",
    "#             print(\"doing else once for \" + str(doneCount))\n",
    "#             doneCount += 1\n",
    "            addNounPhrasesCountField(rev)\n",
    "\n",
    "            for np in rev['nounPhrasesCount']:\n",
    "                productDocumentCount[np] = productDocumentCount.get(np, 0) + rev['nounPhrasesCount'][np]\n",
    "    \n",
    "#     print(productDocument)\n",
    "\n",
    "#print(testDocument)\n",
    "gatherAllReviewsNPCount(testDocument)\n",
    "# print(testDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da93ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(testDocument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899aa2f",
   "metadata": {},
   "source": [
    "### Trying out on the sample of 10\n",
    "finds all documents, performs the `gatherAllReviewsNPCount` on all of them and then outputs to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "baf22c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bson import json_util\n",
    "\n",
    "cursor = joinedSample10Collection.find({})\n",
    "\n",
    "documents = []\n",
    "for document in cursor:\n",
    "    gatherAllReviewsNPCount(document)\n",
    "    documents.append(document)\n",
    "    \n",
    "\n",
    "\n",
    "## outputting    \n",
    "documentsJsonString = json.dumps(documents, default=json_util.default)\n",
    "# # write list of dicts to file\n",
    "with open('outputWithNPCount.json', 'w') as outfile:\n",
    "    outfile.write(documentsJsonString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8494a0a4",
   "metadata": {},
   "source": [
    "## Sentiment generation and average sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dffffcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "\n",
    "def addSentimentToReview(review, lyzer):\n",
    "\n",
    "\n",
    "    if 'reviewText' not in review:\n",
    "        return # have to account for this later\n",
    "    \n",
    "    \n",
    "    testReviewText = review['reviewText']\n",
    "\n",
    "    score = lyzer.polarity_scores(testReviewText)\n",
    "    \n",
    "    review['sentiment'] = score \n",
    "    \n",
    "\n",
    "#before\n",
    "#print(testReview)\n",
    "\n",
    "lyzer = SentimentIntensityAnalyzer()\n",
    "addSentimentToReview(testReview, lyzer)\n",
    "\n",
    "#after\n",
    "#print(testReview)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b95e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentiment(productDocument, lyzer):\n",
    "    reviewsList = productDocument['review']\n",
    "    \n",
    "    averageSentiment = {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "    # need to keep count because some reviews dont have sentiment (because no reviewText)\n",
    "    reviewsWithSentimentCount = 0\n",
    "    \n",
    "    \n",
    "    for rev in reviewsList:\n",
    "        \n",
    "        addSentimentToReview(rev, lyzer)\n",
    "        if 'sentiment' in rev:\n",
    "            reviewsWithSentimentCount += 1\n",
    "            for key in averageSentiment:\n",
    "                averageSentiment[key] += rev['sentiment'][key]\n",
    "    \n",
    "    \n",
    "    for key in averageSentiment:\n",
    "        averageSentiment[key] = averageSentiment[key] / reviewsWithSentimentCount\n",
    "    \n",
    "    productDocument['averageSentiment'] = averageSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1dd0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyzer = SentimentIntensityAnalyzer()\n",
    "generateSentiment(testDocument, lyzer)\n",
    "#print(testDocument)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c03b1",
   "metadata": {},
   "source": [
    "### Trying out with sample of 10\n",
    "Reusing the `documents` list generated earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "82a66f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test first doc\n",
    "# documents[0]\n",
    "\n",
    "for document in documents:\n",
    "    generateSentiment(document, lyzer)\n",
    "    \n",
    "## outputting    \n",
    "documentsJsonString = json.dumps(documents, default=json_util.default)\n",
    "# # write list of dicts to file\n",
    "with open('outputWithNPCountAndSentiment.json', 'w') as outfile:\n",
    "    outfile.write(documentsJsonString)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
